分词系统，参考jieba分词
基于字典分词： 
    基于字典建立Trie树
	    举例 “我们 98740 r”
		    root
		----
		|
		我
		|
		们
		|
		98740
        每一相同层级之间用unordered_map做链接，每一上下层之间用next指针做遍历
	基于动态规划从句尾到句首切分句子，记录当前点的最大组合概率的切分位置
	举例 我们的祖国是花园
		     0 1 2 3 4 5 6 7|
		     0 1 2 3 4 5 6|7
	因为花园是在存储在Trie树的词，可以查到其权重为1291，则1291/total > 1/total * 1/total，那么对于6来说，它的切分位置就是7，即第一种情况。
	
基于HMM分词，Viterbi算法，这个算法通过已知的可观察序列，和已知的状态转移概率，推断出隐含的状态序列的情况：
    中文词汇按照BEMS四个states标记，hmm字典记录了每个字在B,E,M,S四种状态下的发射概率，和四个状态之间的转移概率及各个状态的初始概率。
    建立WordSize*4 path，记录当前state的情况下的max preStat
    建立WordSize*4 weight，记录当前word和state组合下的max prob = preWeight*trans[preState->curStat]*emit[word&curStat]

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
	
句向量构建
基于word2vec训练出每个词的64维词向量表达，基于tfidf计算出每个词在句子中的权重，权重做归一化后乘以每个词向量的结果作为词在当前句子中的词向量
word2vec(我们)*tfidf(我们) + word2vec(的)*tfidf(的) + word2vec(祖国)*tfidf(祖国) + word2vec(是)*tfidf(是) + word2vec(花园)*tfidf(花园)

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

建立 word: doc_id1 doc_id2 doc_id3 ... 的倒排

问题匹配
对待匹配问题做切分，分词后，计算每个包含该分词的文档的tfidf权重和，计算出最大权重的文档列表，权重越大的文档其包含关键词越多越重要，对于相同的
权重的文档，则再计算得到与该句向量的相似度最大的文档句向量，从而得到匹配问题的索引位置。

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

参考资料：
jieba分词：https://github.com/yanyiwu/cppjieba
word2vec: https://github.com/jdeng/word2vec.git

	
    
			 
	
